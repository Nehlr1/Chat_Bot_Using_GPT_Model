{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "import json\n",
    "with open(\"intents.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "patterns = []\n",
    "responses = []\n",
    "\n",
    "for intent in data[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        patterns.append(pattern)\n",
    "        responses.append(intent['responses'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1.550204546220841\n",
      "Epoch: 2, Loss: 0.5390577071136043\n",
      "Epoch: 3, Loss: 0.3518816126931098\n",
      "Epoch: 4, Loss: 0.23296106943199713\n",
      "Epoch: 5, Loss: 0.1582493229258445\n",
      "Epoch: 6, Loss: 0.11154509371807499\n",
      "Epoch: 7, Loss: 0.0869935367616915\n",
      "Epoch: 8, Loss: 0.07047792572167612\n",
      "Epoch: 9, Loss: 0.059371744252500996\n",
      "Epoch: 10, Loss: 0.05564107805971177\n",
      "Epoch: 11, Loss: 0.053503402779179234\n",
      "Epoch: 12, Loss: 0.05051255178066992\n",
      "Epoch: 13, Loss: 0.04794458486139774\n",
      "Epoch: 14, Loss: 0.04415519457430609\n",
      "Epoch: 15, Loss: 0.04088488046921069\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer and model\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Define the dataset\n",
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, questions, answers, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.questions = questions\n",
    "        self.answers = answers\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        answer = self.answers[idx]\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            question, \n",
    "            answer, \n",
    "            add_special_tokens=True, \n",
    "            max_length=self.max_length, \n",
    "            padding='max_length', \n",
    "            truncation='only_first',\n",
    "            return_tensors='pt',\n",
    "            return_overflowing_tokens=True\n",
    "        )\n",
    "\n",
    "        input_ids = inputs['input_ids'].flatten()\n",
    "        if len(input_ids) != self.max_length:\n",
    "           print(f\"Unexpected length at index {idx}: {len(input_ids)}\")\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'labels': inputs['input_ids'].flatten()\n",
    "        }\n",
    "\n",
    "# Training Preparation\n",
    "max_length = 100\n",
    "dataset = ConversationDataset(patterns, responses, tokenizer, max_length=max_length)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, no_deprecation_warning=True)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataloader) * 15)  # Assuming 3 epochs\n",
    "\n",
    "# Training Loop\n",
    "epochs = 15\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    print(f\"Epoch: {epoch + 1}, Loss: {total_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model after trainin\n",
    "model_path = './trained_gpt_model'\n",
    "tokenizer.save_pretrained(model_path)\n",
    "model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./trained_gpt_model\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./trained_gpt_model\")\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))  # Move model to GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(user_input, max_length=50):\n",
    "    # Encode the user's input and add the eos_token\n",
    "    input_ids = tokenizer.encode(user_input, return_tensors='pt', add_special_tokens=True)\n",
    "    input_ids = input_ids.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))  # Move tensor to GPU if available\n",
    "\n",
    "    # Generate a response from the model\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids, pad_token_id=tokenizer.eos_token_id, max_length=max_length, num_return_sequences=1)\n",
    "\n",
    "    # Decode the response\n",
    "    response = tokenizer.decode(output[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Hello, thanks for asking\n",
      "Bot: Please enter with order ID\n",
      "Bot: Yes\n",
      "Bot: Please enter your email id,we will send a link on your email\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in ['quit', 'exit']:\n",
    "        break\n",
    "    response = get_response(user_input)\n",
    "    print(\"Bot:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
